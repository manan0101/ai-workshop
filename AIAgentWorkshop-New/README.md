# ğŸ¤– AI Agent Workshop for Beginners

## Welcome!

**Hello!** This is a simple workshop to learn about AI agents. We'll build smart AI helpers that can work together like a team. No advanced coding experience needed!

## What You'll Learn

- How AI agents work (like smart assistants)
- How to make multiple AI agents work together
- How AI can remember information between steps
- **ğŸ†• Advanced Features**: Intelligent rate limiting and error handling for production-ready AI applications

## Before We Start

You need:

- **Python** (version 3.11 or higher) - most computers already have this!
- **Basic Python knowledge** - if you can write `print("hello")`, you're ready!
- **Internet connection** - to talk to AI services
- **A computer** - Windows, Mac, or Linux

## ğŸš€ Quick Setup (5 minutes!)

### Step 1: Get the Code

```bash
# Download this project
git clone https://github.com/ashishpatel26/AIAgentWorkshop-New.git
cd ai-agent-workshop
```

### Step 2: Install Tools

```bash
# Install the UV package manager (easy Python installer)
# On Windows:
powershell -c "irm https://astral.sh/uv/install.sh | iex"

# On Mac/Linux:
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### Step 3: Install Python Packages

```bash
# Install all needed tools (dependencies are now managed in pyproject.toml)
uv sync
```

### Step 4: Configure Environment

1. **Copy the environment template:**

   ```bash
   cp .env.example .env
   ```
2. **Choose your AI provider and edit `.env`:**

   **For SambaNova (Cloud API - Recommended):**

   ```bash
   # AI Provider Selection
   AI_PROVIDER=sambanova

   # SambaNova API Configuration
   SAMBA_API_KEY=your_sambanova_api_key_here
   SAMBA_MODEL=gpt-oss-120b

   # Workshop Configuration
   WORKSHOP_DEBUG=false
   MAX_TOKENS=4000
   TEMPERATURE=0.7
   ```

   **For Ollama (Local Models - Free):**

   ```bash
   # AI Provider Selection
   AI_PROVIDER=ollama

   # Ollama Configuration
   OLLAMA_MODEL=gemma3:4b

   # Workshop Configuration
   WORKSHOP_DEBUG=false
   MAX_TOKENS=4000
   TEMPERATURE=0.7
   ```

   **For NVIDIA (Cloud API):**

   ```bash
   # AI Provider Selection
   AI_PROVIDER=nvidia

   # NVIDIA API Configuration
   NVIDIA_API_KEY=your_nvidia_api_key_here

   # Workshop Configuration
   WORKSHOP_DEBUG=false
   MAX_TOKENS=4000
   TEMPERATURE=0.7
   ```

### Step 5: Get API Keys

**SambaNova Setup:**

1. Visit [SambaNova](https://sambanova.ai) and create account
2. Get your API key from the dashboard
3. Replace `your_sambanova_api_key_here` in `.env`

**Ollama Setup:**

1. Install Ollama from [ollama.ai](https://ollama.ai)
2. Pull a model: `ollama pull gemma3:4b`
3. Start Ollama: `ollama serve`

**NVIDIA Setup:**

1. Visit [NVIDIA AI Models](https://build.nvidia.com/models) and create an account
2. Go to [API Keys Settings](https://build.nvidia.com/settings/api-keys) to generate your API key
3. Copy the generated API key
4. Replace `your_nvidia_api_key_here` in `.env` with your actual API key

### Step 6: Test Everything Works

```bash
# Test basic imports
python -c "import crewai, langchain_openai; print('âœ… Ready to start!')"

# Test your configuration
uv run python -c "from config import API_KEY, MODEL; print(f'âœ… Config loaded: {MODEL}')"

# Run a simple test (choose based on your provider)
uv run python testing/test_sambanova.py  # For SambaNova
uv run python testing/test_ollama.py      # For Ollama
uv run python testing/test_nvidia_langchain.py  # For NVIDIA
```

## ğŸ¯ What We'll Build (3 Simple Sessions)

### Session 1: Your First AI Agent (30 minutes)

Learn the basics! We'll create:

- A simple AI that can chat
- An AI that can use tools (like a calculator)
- Your first AI "crew" (team)

**Run it:**

```bash
cd session1
uv run basics.py          # Learn basic AI chat
uv run crewai_intro.py    # Learn about AI teams
```

### Session 2: AI Agents Working Together (30 minutes)

Make AI agents collaborate! We'll build:

- Agents with different jobs (researcher, writer)
- A simple content creation team
- How agents share information

**Run it (Command Line):**

```bash
cd session2
uv run agent_roles.py     # See different AI jobs
uv run content_crew.py    # Watch AI create content together
```

**ğŸ¨ Interactive GUIs Available!**

**Session 2 Advanced GUI:**

```bash
# Run the comprehensive multi-team GUI
uv run streamlit run agent_roles_gui.py
```

**GUI Features:**

- ğŸ¨ Beautiful, modern web interfaces
- ğŸ‘¥ Interactive agent team demonstrations
- ğŸ“Š Real-time progress tracking
- ğŸ’¾ Results history and comparison
- ğŸ¯ Educational explanations
- ğŸ”„ Demo mode (works without API keys!)

### Session 3: Smart Workflows (30 minutes)

AI that remembers! We'll create:

- Workflows that pass information between steps
- AI that learns from previous steps
- Simple state management

**Run it:**

```bash
cd session3
uv run stateful_workflow.py  # See AI remember information
```

## ğŸ“ What's In This Project

```bash
ai-agent-workshop/
â”œâ”€â”€ README.md              # This guide (you're reading it!)
â”œâ”€â”€ GIT_SETUP.md          # Git setup instructions for all OS
â”œâ”€â”€ CODE_REVIEW_REPORT.md # Comprehensive code review and recommendations
â”œâ”€â”€ architecture.md       # Detailed codebase architecture documentation
â”œâ”€â”€ ai_agent_workshop_curriculum.md # Workshop curriculum and lesson plans
â”œâ”€â”€ pyproject.toml         # Project configuration and dependencies
â”œâ”€â”€ uv.lock               # Dependency lock file
â”œâ”€â”€ .env.example          # Template for your settings
â”œâ”€â”€ .gitignore           # Git ignore rules
â”œâ”€â”€ config.py             # Simple configuration (loads automatically)
â”œâ”€â”€ session1/             # Basic AI examples
â”‚   â”œâ”€â”€ basics.py         # Your first AI agents
â”‚   â””â”€â”€ crewai_intro.py   # AI working in teams
â”œâ”€â”€ session2/             # AI collaboration
â”‚   â”œâ”€â”€ agent_roles.py    # Different AI jobs
â”‚   â””â”€â”€ content_crew.py   # AI creating content together
â”œâ”€â”€ session3/             # Smart workflows
â”‚   â”œâ”€â”€ stateful_workflow.py          # AI that remembers
â”‚   â”œâ”€â”€ langgraph_basics_nvidia.py    # LangGraph basics with NVIDIA
â”‚   â”œâ”€â”€ stateful_workflow_langchain_nvidia.py # LangChain NVIDIA workflow
â”‚   â””â”€â”€ langgraph_basics.py           # Graph basics
â”œâ”€â”€ testing/              # Test scripts and utilities
â”‚   â”œâ”€â”€ test_langchain.py     # LangChain tests
â”‚   â”œâ”€â”€ test_nvidia_langchain.py  # NVIDIA API tests
â”‚   â”œâ”€â”€ test_nvidia_model.py   # Direct NVIDIA model tests
â”‚   â”œâ”€â”€ test_ollama.py         # Ollama local model tests
â”‚   â””â”€â”€ test_sambanova.py     # SambaNova API tests
â””â”€â”€ utils/                # Helper tools (advanced users only)
    â”œâ”€â”€ config.py         # Legacy configuration
    â”œâ”€â”€ helpers.py        # Utility functions
    â””â”€â”€ rate_limiter.py   # API rate limiting
â”œâ”€â”€ .qodo/                # Project artifacts
```

#### ğŸ—ï¸ Code Architecture Diagram

```mermaid
flowchart TD
    %% Configuration files
    envFile[ğŸ“„ .env<br/>Environment Variables]
    configPy[ğŸ“„ config.py<br/>Simple Auto-Config]
    pyproject[ğŸ“„ pyproject.toml<br/>Dependencies]

    %% Session 1 files
    basics[ğŸ“„ session1/basics.py<br/>Basic Chat & Tools]
    crewaiIntro[ğŸ“„ session1/crewai_intro.py<br/>Agent Teams Intro]

    %% Session 2 files
    agentRoles[ğŸ“„ session2/agent_roles.py<br/>Agent Roles & Tasks]
    contentCrew[ğŸ“„ session2/content_crew.py<br/>Content Creation]

    %% Session 3 files
    statefulWF[ğŸ“„ session3/stateful_workflow.py<br/>Stateful Workflows]
    nvidiaWF[ğŸ“„ session3/langgraph_basics_nvidia.py<br/>LangGraph Basics NVIDIA]
    langchainWF[ğŸ“„ session3/stateful_workflow_langchain_nvidia.py<br/>LangChain Version]

    %% Testing files
    testFiles[ğŸ“ testing/<br/>Test Scripts]

    %% External frameworks
    langchain[(ğŸ¤– LangChain)]
    crewai[(ğŸ‘¥ CrewAI)]
    langgraph[(ğŸ“Š LangGraph)]
    sambanova[(ğŸŒ SambaNova API)]
    ollama[(ğŸ  Ollama Local)]
    nvidia[(ğŸš€ NVIDIA API)]

    %% Connections
    envFile --> configPy
    pyproject --> configPy
    configPy --> basics
    configPy --> crewaiIntro
    configPy --> agentRoles
    configPy --> contentCrew
    configPy --> statefulWF
    configPy --> nvidiaWF
    configPy --> langchainWF
    configPy --> testFiles

    basics --> langchain
    crewaiIntro --> crewai
    agentRoles --> crewai
    contentCrew --> crewai
    statefulWF --> langgraph
    nvidiaWF --> langgraph
    langchainWF --> langchain

    langchain --> sambanova
    langchain --> ollama
    langchain --> nvidia
    crewai --> sambanova
    crewai --> ollama
    crewai --> nvidia
    langgraph --> sambanova
    langgraph --> ollama
    langgraph --> nvidia
```

## ğŸ“Š Individual File Code Architectures

### Session 1: Basic AI Interactions

**File: `session1/basics.py`** - Demonstrates fundamental AI chat and tool usage with LangChain

```mermaid
flowchart TD
    A[ğŸš€ main<br/>Entry Point] --> B[ğŸ’¬ basic_chat_example<br/>Chat Demo]
    A --> C[ğŸ”¢ simple_math_helper<br/>Math Demo]

    B --> D[ğŸ¤– ChatOpenAI<br/>LLM Instance]
    C --> D

    D --> E[âš™ï¸ get_config<br/>Configuration]
    E --> F[ğŸ”§ get_agent_config<br/>Agent Settings]

    F --> G[ğŸ“¡ invoke<br/>API Call]
    G --> H[ğŸ“„ Display Response]
```

**File: `session1/crewai_intro.py`** - Introduction to multi-agent systems with CrewAI

```mermaid
flowchart TD
    A[ğŸš€ main<br/>Entry Point] --> B[ğŸ‘¥ create_simple_crew<br/>Crew Setup]
    B --> C[ğŸ¤– Agent<br/>AI Assistant]
    B --> D[ğŸ“‹ Task<br/>Work Assignment]
    B --> E[ğŸ¯ Crew<br/>Team Orchestrator]

    C --> F[âš™ï¸ get_config<br/>Configuration]
    F --> G[ğŸ”§ get_agent_config<br/>Agent Settings]
    G --> H[ğŸ¤– ChatOpenAI<br/>LLM Instance]

    E --> I[â–¶ï¸ kickoff<br/>Execute Tasks]
    I --> J[ğŸ“Š Display Results]
```

### Session 2: Multi-Agent Collaboration

**File: `session2/agent_roles.py`** - Demonstrates different AI agent roles working together

```mermaid
flowchart TD
    A[ğŸš€ main<br/>Entry Point] --> B[ğŸ“Š demonstrate_agent_roles<br/>Business Demo]
    A --> C[ğŸ³ show_simple_roles<br/>Simple Demo]

    B --> D[â±ï¸ create_rate_limited_llm<br/>Rate Limited LLM]
    C --> D

    D --> E[âš™ï¸ get_config<br/>Configuration]
    E --> F[ğŸ”§ get_agent_config<br/>Agent Settings]
    F --> G[ğŸ¤– ChatOpenAI<br/>LLM Instance]

    B --> H[ğŸ“ˆ Agent<br/>Data Analyst]
    B --> I[ğŸ¯ Agent<br/>Business Strategist]
    B --> J[ğŸ“‹ Task<br/>Analysis Task]
    B --> K[ğŸ“‹ Task<br/>Strategy Task]

    C --> L[ğŸ‘¨â€ğŸ³ Agent<br/>Chef]
    C --> M[ğŸ¥— Agent<br/>Nutritionist]
    C --> N[ğŸ“‹ Task<br/>Recipe Task]
    C --> O[ğŸ“‹ Task<br/>Health Task]

    J --> P[ğŸ‘¥ Crew<br/>Business Crew]
    K --> P
    N --> Q[ğŸ‘¥ Crew<br/>Food Crew]
    O --> Q

    P --> R[â–¶ï¸ kickoff<br/>Execute]
    Q --> R
    R --> S[ğŸ“Š Display Results]
```

**File: `session2/content_crew.py`** - Complete content creation workflow with specialized agents

```mermaid
flowchart TD
    A[ğŸš€ main<br/>Entry Point] --> B[ğŸ“ run_content_creation_workflow<br/>Main Workflow]
    B --> C[ğŸ‘¥ create_content_creation_crew<br/>Agent Setup]
    B --> D[ğŸ“‹ create_content_tasks<br/>Task Setup]

    C --> E[ğŸ” Agent<br/>Researcher]
    C --> F[âœï¸ Agent<br/>Writer]
    C --> G[âœï¸ Agent<br/>Editor]

    D --> H[ğŸ“‹ Task<br/>Research Task]
    D --> I[ğŸ“‹ Task<br/>Writing Task]
    D --> J[ğŸ“‹ Task<br/>Editing Task]

    E --> K[âš™ï¸ get_config<br/>Configuration]
    F --> K
    G --> K
    K --> L[ğŸ”§ get_agent_config<br/>Agent Settings]
    L --> M[ğŸ¤– ChatOpenAI<br/>LLM Instance]

    H --> N[ğŸ‘¥ Crew<br/>Content Crew]
    I --> N
    J --> N

    N --> O[â–¶ï¸ kickoff<br/>Execute Workflow]
    O --> P[ğŸ“„ Display Final Result]
```

### Session 3: Stateful Workflows

**File: `session3/stateful_workflow.py`** - Demonstrates AI workflows that remember information between steps

```mermaid
flowchart TD
    A[ğŸš€ main<br/>Entry Point] --> B[ğŸ”„ run_simple_workflow<br/>Main Demo]
    B --> C[âš™ï¸ create_simple_workflow<br/>Workflow Setup]

    C --> D[ğŸ“Š StateGraph<br/>Workflow Graph]
    C --> E[ğŸ§  WorkflowState<br/>State Definition]

    D --> F[ğŸ” research_step<br/>Research Node]
    D --> G[ğŸ“ draft_answer_step<br/>Draft Node]
    D --> H[âœ… final_answer_step<br/>Final Node]

    F --> I[ğŸ”€ decide_next_step<br/>Router Function]
    G --> I
    H --> I

    I --> J[ğŸ END<br/>Workflow Complete]
    I --> F
    I --> G
    I --> H

    F --> K[ğŸ” Agent<br/>Researcher]
    G --> L[âœï¸ Agent<br/>Writer]
    H --> M[âœï¸ Agent<br/>Editor]

    K --> N[â±ï¸ create_rate_limited_llm<br/>Rate Limited LLM]
    L --> N
    M --> N

    N --> O[âš™ï¸ get_config<br/>Configuration]
    O --> P[ğŸ”§ get_agent_config<br/>Agent Settings]
    P --> Q[ğŸ¤– ChatOpenAI<br/>LLM Instance]

    B --> R[â–¶ï¸ app.invoke<br/>Execute Workflow]
    R --> S[ğŸ“Š Display Results]
```

**File: `session3/langgraph_basics.py`** - Fundamental LangGraph concepts and conditional routing

```mermaid
flowchart TD
    A[ğŸš€ main<br/>Entry Point] --> B[ğŸ§  run_basic_langgraph_example<br/>Basic Example]
    A --> C[ğŸ”€ demonstrate_conditional_routing<br/>Routing Demo]

    B --> D[âš™ï¸ create_langgraph_workflow<br/>Workflow Creation]
    D --> E[ğŸ“Š StateGraph<br/>Graph Builder]
    D --> F[ğŸ§  AgentState<br/>State Definition]

    E --> G[ğŸ” research_node<br/>Research Node]
    E --> H[ğŸ“Š analyze_node<br/>Analysis Node]
    E --> I[ğŸ’¡ answer_node<br/>Answer Node]
    E --> J[ğŸ›ï¸ router_function<br/>Decision Logic]

    G --> K[ğŸ“¡ LLM.invoke<br/>API Call]
    H --> K
    I --> K

    J --> L[ğŸ END<br/>Complete]
    J --> G
    J --> H
    J --> I

    C --> M[ğŸ§  QueryState<br/>State Definition]
    C --> N[ğŸ·ï¸ classify_query<br/>Classification]
    C --> O[ğŸ“ simple_response<br/>Simple Handler]
    C --> P[ğŸ“‹ complex_response<br/>Complex Handler]
    C --> Q[ğŸ¯ route_based_on_complexity<br/>Smart Router]

    N --> R[ğŸ“¡ LLM.invoke<br/>Classify Query]
    O --> S[ğŸ“¡ LLM.invoke<br/>Simple Answer]
    P --> T[ğŸ“¡ LLM.invoke<br/>Complex Answer]

    Q --> U[ğŸ END<br/>Complete]
    Q --> O
    Q --> P

    B --> V[â–¶ï¸ app.invoke<br/>Execute]
    C --> W[â–¶ï¸ app.invoke<br/>Execute]
    V --> X[ğŸ“Š Display Results]
    W --> X
```

### Utils: Helper Modules

**File: `utils/config.py`** - Central configuration management and validation system

```mermaid
flowchart TD
    A[ğŸ”‘ get_config<br/>Global Instance] --> B[âš™ï¸ WorkshopConfig<br/>Main Class]
    B --> C[ğŸ“¥ _load_config<br/>Load Settings]
    B --> D[ğŸ”„ _convert_types<br/>Type Conversion]
    B --> E[âœ… validate<br/>Configuration Check]

    C --> F[ğŸ“‚ load_environment_variables<br/>From helpers.py]
    F --> G[ğŸ“„ load_dotenv<br/>Load .env file]
    F --> H[ğŸ” validate_api_key<br/>Key Validation]

    B --> I[ğŸ¤– get_agent_config<br/>Agent Settings]
    B --> J[ğŸ”„ get_workflow_config<br/>Workflow Settings]
    B --> K[ğŸ’¾ save_to_env_file<br/>Persist Config]

    I --> L[ğŸ·ï¸ openrouter/model<br/>Model Prefix]
    J --> M[â±ï¸ timeout/debug<br/>Workflow Params]

    E --> N[ğŸ”‘ API Key Check]
    E --> O[ğŸ¤– Model Validation]
    E --> P[ğŸŒ¡ï¸ Temperature Range]
    E --> Q[ğŸ”¢ Token Limits]
```

**File: `utils/helpers.py`** - Utility functions for environment handling and data processing

```mermaid
flowchart TD
    A[ğŸ“‚ load_environment_variables<br/>Env Loading] --> B[ğŸ“„ load_dotenv<br/>Load .env]
    A --> C[ğŸ” validate_api_key<br/>Key Validation]
    A --> D[ğŸ“Š Return Dict<br/>Env Variables]

    E[ğŸ“ format_agent_response<br/>Response Formatting] --> F[ğŸ”¤ String Check]
    E --> G[ğŸ”„ Object Conversion]
    E --> H[ğŸ§¹ Clean Output]

    I[ğŸ“Š create_progress_indicator<br/>Progress Bar] --> J[ğŸ”¢ Calculate Percentage]
    I --> K[â–¬ Create Bar String]
    I --> L[ğŸ“„ Return Formatted String]

    M[ğŸ›¡ï¸ safe_get_nested_value<br/>Safe Dict Access] --> N[ğŸ” Try Key Access]
    M --> O[âš ï¸ Exception Handling]
    M --> P[ğŸ”™ Return Default]

    Q[âœ‚ï¸ truncate_text<br/>Text Truncation] --> R[ğŸ“ Length Check]
    Q --> S[â• Add Suffix]
    Q --> T[ğŸ“„ Return Truncated]

    U[ğŸ“‹ format_workflow_summary<br/>Summary Creation] --> V[ğŸ“Š Extract State Data]
    U --> W[ğŸ“ Format Lines]
    U --> X[ğŸ“„ Return Summary]

    Y[ğŸ¤– get_available_models<br/>Model List] --> Z[ğŸ“‹ Return Model Array]
    AA[ğŸ’° estimate_cost<br/>Cost Calculation] --> BB[ğŸ”¢ Token Estimation]
    AA --> CC[ğŸ” Cost Lookup]
    AA --> DD[ğŸ’µ Return Cost]
```

**File: `utils/rate_limiter.py`** - Intelligent API rate limiting and retry logic

```mermaid
flowchart TD
    A[ğŸ›¡ï¸ RateLimiter<br/>Main Class] --> B[ğŸš€ __init__<br/>Initialize]
    A --> C[â±ï¸ _calculate_delay<br/>Delay Calculation]
    A --> D[ğŸ” _extract_retry_after<br/>Header Parsing]
    A --> E[ğŸš¨ _is_rate_limit_error<br/>Error Detection]
    A --> F[ğŸ”„ call_with_retry<br/>Retry Logic]

    B --> G[ğŸ”¢ max_retries<br/>Retry Count]
    B --> H[â±ï¸ base_delay<br/>Base Delay]
    B --> I[â±ï¸ max_delay<br/>Max Delay]

    C --> J[ğŸ“ˆ Exponential Backoff<br/>2^attempt]
    C --> K[ğŸ² Add Jitter<br/>Â±25%]
    C --> L[ğŸ›‘ Cap at Max<br/>Delay Limit]

    D --> M[ğŸ” Regex Search<br/>X-RateLimit-Reset]
    D --> N[ğŸ“… Timestamp Parse]
    D --> O[ğŸ§® Calculate Delay]

    E --> P[ğŸ”¤ Error String Check]
    E --> Q[ğŸ¯ Keyword Match<br/>rate limit, 429, etc.]

    F --> R[ğŸ” Retry Loop<br/>max_retries + 1]
    F --> S[â±ï¸ Rate Limiting<br/>Min Interval]
    F --> T[âš ï¸ Exception Handling]
    F --> U[â±ï¸ Delay Calculation]
    F --> V[âœ… Success Return]

    W[ğŸ­ create_rate_limited_llm<br/>Factory Function] --> X[ğŸ›¡ï¸ RateLimiter<br/>Instance]
    W --> Y[ğŸ¤– ChatOpenAI<br/>LLM Creation]
    W --> Z[ğŸ¤– Return LLM<br/>With Retry Logic]
```

## ğŸ†˜ Having Problems?

### "API Key Not Working"

**For SambaNova:**

- Check your `.env` file has the correct key from SambaNova dashboard
- Make sure `AI_PROVIDER=sambanova` is set
- Verify your SambaNova account has credits

**For Ollama:**

- Make sure Ollama is running: `ollama serve`
- Check that your model is pulled: `ollama list`
- Verify `AI_PROVIDER=ollama` and correct model name in `.env`

**For NVIDIA:**

- Check your `.env` file has the correct key from NVIDIA API dashboard
- Make sure `AI_PROVIDER=nvidia` is set
- Verify your NVIDIA account has credits and API access

### "Model Not Found" or "404 Error"

- For SambaNova: Check available models at [SambaNova Models](https://sambanova.ai)
- For Ollama: Pull the model first: `ollama pull gemma3:4b`
- For NVIDIA: Check available models at [NVIDIA API Models](https://build.nvidia.com)
- Update your `.env` file with the correct model name

### "Connection Failed"

**SambaNova:**

- Check internet connection
- Verify API key is active
- Try a different model

**Ollama:**

- Ensure Ollama is running on http://localhost:11434
- Check: `curl http://localhost:11434/api/tags`
- Restart Ollama if needed

**NVIDIA:**

- Check internet connection
- Verify API key is active and has sufficient credits
- Try a different model if available

### "Package Installation Failed"

```bash
# Try reinstalling dependencies
uv sync --reinstall
```

### "Python Not Found"

- Download Python from python.org (version 3.8+)
- Make sure `python` command works in terminal

### Still Stuck?

- Check that all files are in the right folders
- Try running: `python -c "print('Python works!')"`
- Ask for help - you're learning something new! ğŸš€

## ğŸ‰ You're Done!

**Congratulations!** You've learned about AI agents. What you built:

- ğŸ¤– AI that can chat and use tools
- ğŸ‘¥ AI agents working as a team
- ğŸ§  AI that remembers information between steps
- âš¡ **Production-ready features**: Intelligent rate limiting, error handling, and API resilience

## Next Steps

Ready for more? Try:

- Change the questions in the examples
- Add your own AI agents
- Build something fun with what you learned!

---

**Happy AI Building!** ğŸš€ğŸ¤–
